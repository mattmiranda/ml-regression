{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset into a pandas dataframe\n",
    "df = pd.read_excel('./data/kimchi_dataset.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types in dataset\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if XLarge Boxes has any meaningful data\n",
    "df['XLarge Boxes'].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the integrity of the \"Total Boxes\" column, is it really the sum of all the boxes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_boxes = df['Small Boxes'] + df['Large Boxes'] + df['XLarge Boxes']\n",
    "equals = df['Total Boxes'].equals(total_boxes)\n",
    "equals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df['Total Boxes'].compare(total_boxes)\n",
    "diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference. The reason the equals method returns False is probably due to the way the float values are stored which makes them non-identical but essentially the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to make it easier to access\n",
    "df = df.rename(columns={\"Total Volume\": \"Volume\", \"Total Boxes\": \"Boxes_T\", \"Small Boxes\": \"Boxes_S\", \"Large Boxes\": \"Boxes_L\", \"XLarge Boxes\": \"Boxes_XL\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows\n",
    "duplicate_rows_df = df[df.duplicated()]\n",
    "print(\"number of duplicated rows: \", duplicate_rows_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that number of items match for all columns\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values (null or na)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 4 missing values for price and 1 for volume. Since 5 is a small portion of the data I will simply drop the rows with missing values.\n",
    "df = df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is clean (no missing values) we can continue with EDA by looking for outliers and skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(df['Price'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.distplot(df['Volume'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(df['Boxes_S'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.distplot(df['Boxes_L'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outliers using boxplot\n",
    "plt.figure(figsize=[16, 8])\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(x=df['Price'])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=df['Volume'])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(x=df['Boxes_S'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=df['Boxes_L'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see in the plots above, the data is quite skewed.\n",
    "# Try removing some outliers using the Interquartile Range (IQR) technique that is suitable for skewed data.\n",
    "# sub_df = df[[\"Price\", \"Volume\", \"Boxes_T\", \"Boxes_S\", \"Boxes_L\", \"Boxes_XL\"]]\n",
    "Q1 = df[\"Price\"].quantile(0.25)\n",
    "Q3 = df[\"Price\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding lower and upper limits\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "lower_limit = Q1 - 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding outliers\n",
    "df[df['Price'] < lower_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no outliers lower than the 25 percentile range. Focusing on the upper limit. \n",
    "df[df['Price'] > upper_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming outliers\n",
    "trim_df = df[df['Price'] < upper_limit]\n",
    "trim_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare plots after trimming\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(x=df['Price'])\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=df['Price'])\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(x=trim_df['Price'])\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=trim_df['Price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping\n",
    "df_cap = df.copy()\n",
    "df_cap['Price'] = np.where(\n",
    "    df_cap['Price'] > upper_limit,\n",
    "    upper_limit,\n",
    "    np.where(\n",
    "        df_cap['Price'] < lower_limit,\n",
    "        lower_limit,\n",
    "        df_cap['Price']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare plots after capping\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.distplot(x=df['Price'])\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=df['Price'])\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(x=df_cap['Price'])\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=df_cap['Price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing other features\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(2,2,1)\n",
    "sns.boxplot(x=df_cap['Volume'])\n",
    "plt.subplot(2,2,2)\n",
    "sns.boxplot(x=df_cap['Boxes_S'])\n",
    "plt.subplot(2,2,3)\n",
    "sns.boxplot(x=df_cap['Boxes_L'])\n",
    "plt.subplot(2,2,4)\n",
    "sns.boxplot(x=df_cap['Boxes_XL'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the other columns are also very skewed (right-skewed), I'll not trim or cap them because doing so will reduce the dataset size that is already quite small. \n",
    "There are 2 options here: \n",
    "1. Transform data to approximate normal distribution.\n",
    "2. Use a Standard Scaler and don't fix skew.\n",
    "2. Use tree based model for regression as they are not affected by skewed data.\n",
    "\n",
    "For transformation, there are many options\n",
    "1. log transformation\n",
    "2. Normalize (min-max)\n",
    "3. Cube root: used when values are too large. Can be applied to negative values.\n",
    "4. Square root: Applied only to positive values.\n",
    "5. Reciprocal\n",
    "6. Square: apply on left skew (not this case)\n",
    "7. Box Cox transformation\n",
    "8. Quantile transform\n",
    "9. Power transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try Power transform and quantile transform\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "cols1 = [\"Price\", \"Volume\", \"Boxes_S\"]\n",
    "def test_transformers(columns):\n",
    "    pt = PowerTransformer()\n",
    "    qt = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "    fig = plt.figure(figsize=(16,24))\n",
    "    j = 1\n",
    "    for i in columns:\n",
    "        array = np.array(df[i]).reshape(-1, 1)\n",
    "        y = pt.fit_transform(array)\n",
    "        x = qt.fit_transform(array)\n",
    "        plt.subplot(3,3,j)\n",
    "        sns.histplot(array, bins = 50, kde = True)\n",
    "        plt.title(f\"Original Distribution for {i}\")\n",
    "        plt.subplot(3,3,j+1)\n",
    "        sns.histplot(x, bins = 50, kde = True)\n",
    "        plt.title(f\"Quantile Transform for {i}\")\n",
    "        plt.subplot(3,3,j+2)\n",
    "        sns.histplot(y, bins = 50, kde = True)\n",
    "        plt.title(f\"Power Transform for {i}\")\n",
    "        j += 3\n",
    "test_transformers(cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile Transform is able to normalize the data nicely so I will use that. Remember that the same preprocessing must be applied to new data later. \n",
    "# I'm dropping \"Total Boxes\" because it won't make sense here.\n",
    "df = df.drop([\"Boxes_T\"], axis=1)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = df.copy()\n",
    "cols2 = [\"Price\", \"Volume\", \"Boxes_S\", \"Boxes_L\", \"Boxes_XL\"]\n",
    "def transform_columns(columns):\n",
    "    qt = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "    for i in columns:\n",
    "        array = np.array(df[i]).reshape(-1, 1)\n",
    "        df_trans[i] = qt.fit_transform(array)\n",
    "transform_columns(cols2)\n",
    "df_trans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Histogram\n",
    "plt.figure(figsize=[10, 5])\n",
    "df_trans['Region'].value_counts().plot(kind=\"bar\", title=\"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column into separate features\n",
    "df_trans.Date = pd.to_datetime(df_trans.Date)\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans[\"year\"] = df_trans.Date.dt.year\n",
    "df_trans[\"month\"] = df_trans.Date.dt.month\n",
    "df_trans[\"workingday\"] = np.where(df_trans.Date.dt.dayofweek < 5, True, False)\n",
    "df_trans[\"season\"] = df_trans.apply(lambda x: 'winter' if x['month'] in [1,2,12] else ('spring' if x['month'] in [3,4,5] else ('summer' if x['month'] in [6,7,8] else 'autumm')), axis=1)\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans.Date.dt.dayofweek.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dates are Sundays and year is 2018, so I'll drop the columns.\n",
    "df_trans = df_trans.drop([\"workingday\", \"year\", \"Date\"], axis=1)\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "plt.figure(figsize=(10,5))\n",
    "c = df_trans[[\"Price\", \"Volume\", \"Boxes_S\", \"Boxes_L\", \"Boxes_XL\",\"month\"]].corr()\n",
    "sns.heatmap(c, cmap=\"BrBG\", annot=True)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between volume and Boxes_S, maybe most of the volume comes from small boxes.\n",
    "# Price is slightly correlated to Boxes_XL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for correlation between region and price\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=df_trans[\"Region\"], y=df_trans[\"Price\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# The dataset has some categorical variables that need to be encoded.\n",
    "# Nominal: Region\n",
    "# Ordinal: Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding ordinal variable \"season\"\n",
    "df_trans[\"season\"] = df_trans[\"season\"].map({ \"winter\": 0, \"spring\": 1, \"summer\": 2, \"autumm\": 3})\n",
    "df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding nominal variabe \"Region\"\n",
    "one_hot_enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc.fit_transform(df_trans[[\"Region\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into training and testing sets\n",
    "X = df_trans.drop([\"Price\"], axis=1)\n",
    "y = df_trans[\"Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make column transformer to make it easier to create scikit-learn pipeline later\n",
    "# Encoding nominal variabe \"Region\"\n",
    "one_hot_enc = OneHotEncoder(sparse=False)\n",
    "col_trans = make_column_transformer((one_hot_enc, [\"Region\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_trans.fit_transform(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines from all 3 models\n",
    "\n",
    "# Instantiate pipeline with linear regression\n",
    "lm = LinearRegression()\n",
    "lm_pipeline = make_pipeline(col_trans, lm)\n",
    "\n",
    "# Instantiate Gradient Boosting Regressor with default loss = 'squared_error' and default criterion = 'friedman_mse'\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(col_trans, gbm)\n",
    "\n",
    "# Deault criterion for Random Forrest is 'squared_error'\n",
    "rfm = RandomForestRegressor()\n",
    "rfm_pipeline = make_pipeline(col_trans, rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "lm_pipeline.fit(X_train, y_train)\n",
    "lm_predictions = lm_pipeline.predict(X_test)\n",
    "print(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n",
    "\n",
    "gbm_pipeline.fit(X_train, y_train)\n",
    "gbm_predictions = gbm_pipeline.predict(X_test)\n",
    "print(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))\n",
    "\n",
    "rfm_pipeline.fit(X_train, y_train)\n",
    "rfm_predictions = rfm_pipeline.predict(X_test)\n",
    "print(\"First 5 RFM predictions: \", list(rfm_predictions[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error (MAE) and root mean squared error (RMSE)\n",
    "\n",
    "lm_mae = mean_absolute_error(y_true=y_test, y_pred=lm_predictions)\n",
    "lm_rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=lm_predictions))\n",
    "print(f\"LM MAE: {round(lm_mae, 2)}\")\n",
    "print(f\"LM RMSE: {round(lm_rmse, 2)}\")\n",
    "\n",
    "gbm_mae = mean_absolute_error(y_true=y_test, y_pred=gbm_predictions)\n",
    "gbm_rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=gbm_predictions))\n",
    "print(f\"GBM MAE: {round(gbm_mae, 2)}\")\n",
    "print(f\"GBM RMSE: {round(gbm_rmse, 2)}\")\n",
    "\n",
    "rfm_mae = mean_absolute_error(y_true=y_test, y_pred=rfm_predictions)\n",
    "rfm_rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=rfm_predictions))\n",
    "print(f\"RFM MAE: {round(rfm_mae, 2)}\")\n",
    "print(f\"RFM RMSE: {round(rfm_rmse, 2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More streamlined approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Boxes_T</th>\n",
       "      <th>Boxes_S</th>\n",
       "      <th>Boxes_L</th>\n",
       "      <th>Boxes_XL</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2321.82</td>\n",
       "      <td>2006.46</td>\n",
       "      <td>1996.46</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3154.45</td>\n",
       "      <td>2580.60</td>\n",
       "      <td>2577.27</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2570.52</td>\n",
       "      <td>2209.29</td>\n",
       "      <td>2209.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.48</td>\n",
       "      <td>3851.30</td>\n",
       "      <td>3242.98</td>\n",
       "      <td>3239.65</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.56</td>\n",
       "      <td>5356.63</td>\n",
       "      <td>4007.48</td>\n",
       "      <td>4007.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>1.63</td>\n",
       "      <td>17074.83</td>\n",
       "      <td>13498.67</td>\n",
       "      <td>13066.82</td>\n",
       "      <td>431.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boryeong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>2018-01-28</td>\n",
       "      <td>1.71</td>\n",
       "      <td>13888.04</td>\n",
       "      <td>9264.84</td>\n",
       "      <td>8940.04</td>\n",
       "      <td>324.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boryeong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>2018-01-21</td>\n",
       "      <td>1.87</td>\n",
       "      <td>13766.76</td>\n",
       "      <td>9394.11</td>\n",
       "      <td>9351.80</td>\n",
       "      <td>42.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boryeong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2018-01-14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>16205.22</td>\n",
       "      <td>10969.54</td>\n",
       "      <td>10919.54</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boryeong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>1.62</td>\n",
       "      <td>17489.58</td>\n",
       "      <td>12014.15</td>\n",
       "      <td>11988.14</td>\n",
       "      <td>26.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Boryeong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Price    Volume   Boxes_T   Boxes_S  Boxes_L  Boxes_XL   \n",
       "0   2018-03-25   1.71   2321.82   2006.46   1996.46    10.00       0.0  \\\n",
       "1   2018-03-18   1.66   3154.45   2580.60   2577.27     3.33       0.0   \n",
       "2   2018-03-11   1.68   2570.52   2209.29   2209.29     0.00       0.0   \n",
       "3   2018-03-04   1.48   3851.30   3242.98   3239.65     3.33       0.0   \n",
       "4   2018-02-25   1.56   5356.63   4007.48   4007.48     0.00       0.0   \n",
       "..         ...    ...       ...       ...       ...      ...       ...   \n",
       "643 2018-02-04   1.63  17074.83  13498.67  13066.82   431.85       0.0   \n",
       "644 2018-01-28   1.71  13888.04   9264.84   8940.04   324.80       0.0   \n",
       "645 2018-01-21   1.87  13766.76   9394.11   9351.80    42.31       0.0   \n",
       "646 2018-01-14   1.93  16205.22  10969.54  10919.54    50.00       0.0   \n",
       "647 2018-01-07   1.62  17489.58  12014.15  11988.14    26.01       0.0   \n",
       "\n",
       "       Region  \n",
       "0       Seoul  \n",
       "1       Seoul  \n",
       "2       Seoul  \n",
       "3       Seoul  \n",
       "4       Seoul  \n",
       "..        ...  \n",
       "643  Boryeong  \n",
       "644  Boryeong  \n",
       "645  Boryeong  \n",
       "646  Boryeong  \n",
       "647  Boryeong  \n",
       "\n",
       "[644 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date column into separate features\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df.Date.dt.month\n",
    "df[\"season\"] = df.apply(lambda x: 0 if x['month'] in [1,2,12] else (1 if x['month'] in [3,4,5] else (2 if x['month'] in [6,7,8] else 4)), axis=1)\n",
    "df = df.drop([\"Date\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into training and testing sets\n",
    "X = df.drop([\"Price\", \"Boxes_T\"], axis=1)\n",
    "y = df[\"Price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name, pipeline, X_train, y_train, X_test, y_test):\n",
    "    # Train model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    mae = mean_absolute_error(y_true=y_test, y_pred=predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=predictions))\n",
    "    print(f\"{model_name} MAE: {round(mae, 2)}\")\n",
    "    print(f\"{model_name} RMSE: {round(rmse, 2)}\")\n",
    "    return round(rmse, 2)\n",
    "\n",
    "def run_models(models, X_train, y_train, X_test, y_test):\n",
    "    mae = []\n",
    "    rmse = []\n",
    "    names = []\n",
    "    for item in models:\n",
    "        pipeline = item[\"pipeline\"]\n",
    "        names.append(item[\"name\"])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        predictions = pipeline.predict(X_test)\n",
    "        mae.append(mean_absolute_error(y_true=y_test, y_pred=predictions))\n",
    "        rmse.append(np.sqrt(mean_squared_error(y_true=y_test, y_pred=predictions)))\n",
    "    col={\"MAE\": mae, \"RMSE\": rmse}\n",
    "    ret_df = pd.DataFrame(data=col, index=names)\n",
    "    return ret_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.34245123, -0.33680131, -0.30551989, -0.19348517],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.33854283, -0.33176289, -0.30574451, -0.19348517],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.34128382, -0.33495505, -0.30585664, -0.19348517],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.33527179, -0.32601687, -0.30574451, -0.19348517],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.32820571, -0.31935609, -0.30585664, -0.19348517]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make column transformer to make it easier to create scikit-learn pipeline later\n",
    "# Encoding nominal variabe \"Region\"\n",
    "one_hot_enc = OneHotEncoder(sparse=False)\n",
    "scaler = StandardScaler()\n",
    "col_trans = make_column_transformer((one_hot_enc, [\"Region\"]), (scaler, [\"Volume\", \"Boxes_S\", \"Boxes_L\", \"Boxes_XL\"]))\n",
    "col_trans.fit_transform(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearReg</th>\n",
       "      <td>10.342081</td>\n",
       "      <td>41.274891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradBoost</th>\n",
       "      <td>46.800651</td>\n",
       "      <td>373.293766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandFor</th>\n",
       "      <td>32.443142</td>\n",
       "      <td>227.896476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecTree</th>\n",
       "      <td>7.421035</td>\n",
       "      <td>45.788963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>5.667163</td>\n",
       "      <td>53.195512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.117898</td>\n",
       "      <td>0.160601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MAE        RMSE\n",
       "LinearReg  10.342081   41.274891\n",
       "GradBoost  46.800651  373.293766\n",
       "RandFor    32.443142  227.896476\n",
       "DecTree     7.421035   45.788963\n",
       "KNN         5.667163   53.195512\n",
       "SVM         0.117898    0.160601"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipelines from all models\n",
    "\n",
    "models = []\n",
    "\n",
    "# Instantiate pipeline with linear regression\n",
    "lm = LinearRegression()\n",
    "lm_pipeline = make_pipeline(col_trans, lm)\n",
    "# run_model(\"Linear\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"LinearReg\", \"pipeline\": lm_pipeline})\n",
    "\n",
    "# Instantiate Gradient Boosting Regressor with default loss = 'squared_error' and default criterion = 'friedman_mse'\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(col_trans, gbm)\n",
    "# run_model(\"GB\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"GradBoost\", \"pipeline\": gbm_pipeline})\n",
    "\n",
    "# Deault criterion for Random Forrest is 'squared_error'\n",
    "rfm = RandomForestRegressor()\n",
    "rfm_pipeline = make_pipeline(col_trans, rfm)\n",
    "# run_model(\"RandForr\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"RandFor\", \"pipeline\": rfm_pipeline})\n",
    "\n",
    "# Decision Tree\n",
    "dtm = tree.DecisionTreeRegressor(max_depth=1)\n",
    "dtm_pipeline = make_pipeline(col_trans, dtm)\n",
    "# run_model(\"DecTree\", dtm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"DecTree\", \"pipeline\": dtm_pipeline})\n",
    "\n",
    "# KNN\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform')\n",
    "knn_pipeline = make_pipeline(col_trans, knn)\n",
    "# run_model(\"KNN\", knn_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"KNN\", \"pipeline\": knn_pipeline})\n",
    "\n",
    "# SVM\n",
    "svm = SVR()\n",
    "svm_pipeline = make_pipeline(col_trans, svm)\n",
    "# run_model(\"SVM\", svm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"SVM\", \"pipeline\": svm_pipeline})\n",
    "\n",
    "results = run_models(models, X_train, y_train, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING QUANTILE TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create pipeline with Transform (normalizer) instead and compare results. Do no normalize price.\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -2.50462887, -1.32807964, -0.77070507, -5.19933758],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -1.85471132, -1.08802674, -1.06566933, -5.19933758],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -2.15967771, -1.22305738, -5.19933758, -5.19933758],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -1.68285404, -0.90685553, -1.06566933, -5.19933758],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -1.33951127, -0.80076455, -5.19933758, -5.19933758]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make column transformer to make it easier to create scikit-learn pipeline later\n",
    "# Encoding nominal variabe \"Region\"\n",
    "one_hot_enc = OneHotEncoder(sparse=False)\n",
    "quant_trans = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "col_trans = make_column_transformer((one_hot_enc, [\"Region\"]), (quant_trans, [\"Volume\", \"Boxes_S\", \"Boxes_L\", \"Boxes_XL\"]))\n",
    "col_trans.fit_transform(X)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/mattmiranda/Documents/ml-regression/.venv/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearReg</th>\n",
       "      <td>16.655392</td>\n",
       "      <td>43.831407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradBoost</th>\n",
       "      <td>46.804889</td>\n",
       "      <td>373.293766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandFor</th>\n",
       "      <td>33.929812</td>\n",
       "      <td>246.927659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecTree</th>\n",
       "      <td>7.421035</td>\n",
       "      <td>45.788963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>14.366465</td>\n",
       "      <td>91.609868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.121549</td>\n",
       "      <td>0.165607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MAE        RMSE\n",
       "LinearReg  16.655392   43.831407\n",
       "GradBoost  46.804889  373.293766\n",
       "RandFor    33.929812  246.927659\n",
       "DecTree     7.421035   45.788963\n",
       "KNN        14.366465   91.609868\n",
       "SVM         0.121549    0.165607"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipelines from all models\n",
    "\n",
    "models = []\n",
    "\n",
    "# Instantiate pipeline with linear regression\n",
    "lm = LinearRegression()\n",
    "lm_pipeline = make_pipeline(col_trans, lm)\n",
    "# run_model(\"Linear\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"LinearReg\", \"pipeline\": lm_pipeline})\n",
    "\n",
    "# Instantiate Gradient Boosting Regressor with default loss = 'squared_error' and default criterion = 'friedman_mse'\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_pipeline = make_pipeline(col_trans, gbm)\n",
    "# run_model(\"GB\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"GradBoost\", \"pipeline\": gbm_pipeline})\n",
    "\n",
    "# Deault criterion for Random Forrest is 'squared_error'\n",
    "rfm = RandomForestRegressor()\n",
    "rfm_pipeline = make_pipeline(col_trans, rfm)\n",
    "# run_model(\"RandForr\", lm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"RandFor\", \"pipeline\": rfm_pipeline})\n",
    "\n",
    "# Decision Tree\n",
    "dtm = tree.DecisionTreeRegressor(max_depth=1)\n",
    "dtm_pipeline = make_pipeline(col_trans, dtm)\n",
    "# run_model(\"DecTree\", dtm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"DecTree\", \"pipeline\": dtm_pipeline})\n",
    "\n",
    "# KNN\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=5, weights='uniform')\n",
    "knn_pipeline = make_pipeline(col_trans, knn)\n",
    "# run_model(\"KNN\", knn_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"KNN\", \"pipeline\": knn_pipeline})\n",
    "\n",
    "# SVM\n",
    "svm = SVR()\n",
    "svm_pipeline = make_pipeline(col_trans, svm)\n",
    "# run_model(\"SVM\", svm_pipeline, X_train, y_train, X_test, y_test)\n",
    "models.append({\"name\": \"SVM\", \"pipeline\": svm_pipeline})\n",
    "\n",
    "results = run_models(models, X_train, y_train, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
